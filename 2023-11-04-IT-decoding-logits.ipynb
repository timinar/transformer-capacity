{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inar/.local/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unembedding matrix and its pseudo-inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Where is it documented?\n",
    "# Check if works for all models \n",
    "unembed = model.get_output_embeddings().weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, hidden_dim = unembed.shape\n",
    "vocab_size, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross check\n",
    "test = 'Tom and Lily were playing with their toys in the living room. They liked to build towers and bridges with their blocks and cars. Tom was very proud of his tall tower. '\n",
    "inputs = tokenizer(test, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"], output_hidden_states=True)\n",
    "\n",
    "test_logits = outputs.logits.detach()\n",
    "explicit_logits = outputs.hidden_states[-1] @ unembed.T\n",
    "\n",
    "assert torch.allclose(test_logits, explicit_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding earlier layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([0.9272, 0.0592, 0.0056], grad_fn=<TopkBackward0>),\n",
       " indices=tensor([198, 383, 262])),\n",
       " torch.return_types.topk(\n",
       " values=tensor([0.2535, 0.1819, 0.1436]),\n",
       " indices=tensor([ 13,  11, 618])),\n",
       " torch.return_types.topk(\n",
       " values=tensor([0.1807, 0.1396, 0.0722]),\n",
       " indices=tensor([  198, 20037,  1119])))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 12\n",
    "l0 = F.softmax(outputs.hidden_states[3] @ unembed.T, dim=-1)[0,idx]\n",
    "lfin_prev = F.softmax(test_logits[0,idx-1], dim=-1)\n",
    "lfin = F.softmax(test_logits[0,idx], dim=-1)\n",
    "\n",
    "torch.topk(l0, 3), torch.topk(lfin_prev, 3), torch.topk(lfin, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_predictions(outputs, unembed, token_idx, layers, k=3):\n",
    "    \"\"\"\n",
    "    Print the top-k predictions for a specific token at multiple layers, as well as the actual predictions\n",
    "    for the current and previous tokens from the final layer.\n",
    "\n",
    "    Args:\n",
    "    outputs (torch.Tensor): The model outputs that include hidden_states.\n",
    "    unembed (torch.Tensor): The detached weights of the output embeddings.\n",
    "    token_idx (int): The index of the token to analyze.\n",
    "    layers (List[int]): The layers from which to take the hidden states.\n",
    "    k (int): The number of top predictions to return.\n",
    "    \"\"\"\n",
    "    num_layers = len(outputs.hidden_states)\n",
    "\n",
    "    # Check if all layers are within range\n",
    "    for layer in layers:\n",
    "        if layer >= num_layers or layer < 0:\n",
    "            raise ValueError(f\"Layer {layer} is out of bounds for model with {num_layers} layers.\")\n",
    "    \n",
    "    # Calculate the logits for the final layer\n",
    "    logits_final = F.softmax(outputs.hidden_states[-1] @ unembed.T, dim=-1)\n",
    "    \n",
    "    # Get the top-k tokens and their probabilities for the final layer at the current and previous token\n",
    "    topk_final_current = torch.topk(logits_final[0, token_idx], k)\n",
    "    topk_final_previous = torch.topk(logits_final[0, token_idx - 1], k) if token_idx > 0 else None\n",
    "\n",
    "    # Formatting function\n",
    "    def format_topk(topk):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(topk.indices.tolist())\n",
    "        probabilities = topk.values.tolist()\n",
    "        return ', '.join([f\"{token:10}: {prob:.4f}\" for token, prob in zip(tokens, probabilities)])\n",
    "\n",
    "    print(f'Token Index {token_idx} Top {k}')\n",
    "    # Process each requested layer\n",
    "    for layer in layers:\n",
    "        hidden_states_layer = outputs.hidden_states[layer]\n",
    "        logits_layer = F.softmax(hidden_states_layer @ unembed.T, dim=-1)\n",
    "        topk_layer = torch.topk(logits_layer[0, token_idx], k)\n",
    "        formatted_layer = format_topk(topk_layer)\n",
    "\n",
    "        print(f\"Layer {layer:3}:  {formatted_layer}\")\n",
    "        if layer == 0:\n",
    "            if topk_final_previous:\n",
    "                formatted_final_previous = format_topk(topk_final_previous)\n",
    "                print(f\"F Prev Tok: {formatted_final_previous}\")\n",
    "            else:\n",
    "                print(\"No predictions for the previous token: It is the first token in the sequence.\")\n",
    "\n",
    "    # Format and print the final layer predictions\n",
    "    formatted_final_current = format_topk(topk_final_current)\n",
    "    print(f\"\\nFinal:      {formatted_final_current}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_sentence = \"Tom and Lily were playing with their toys in the living room. They liked to build towers and bridges with their blocks and cars.\"\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"], output_hidden_states=True)\n",
    "unembed = model.get_output_embeddings().weight.detach()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_index_of_interest = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current token:  to\n",
      "target  token:  build\n",
      " liked to build\n",
      "\n",
      "\n",
      "Token Index 15 Top 3\n",
      "Layer   0:  Ġto       : 0.0036, to        : 0.0007, Ġin       : 0.0006\n",
      "F Prev Tok: Ġthe      : 0.2205, Ġto       : 0.1781, Ġit       : 0.0948\n",
      "Layer   1:  Ġthe      : 0.9984, Ġa        : 0.0011, Ġ\"        : 0.0003\n",
      "Layer   2:  Ġthe      : 0.9856, Ġbe       : 0.0101, Ġget      : 0.0012\n",
      "Layer   3:  Ġthe      : 0.7298, Ġbe       : 0.1872, Ġmake     : 0.0454\n",
      "Layer   4:  Ġthe      : 0.3626, Ġbe       : 0.3429, Ġmake     : 0.1533\n",
      "Layer   5:  Ġbe       : 0.6678, Ġmake     : 0.2545, Ġkeep     : 0.0304\n",
      "Layer   6:  Ġbe       : 0.7680, Ġmake     : 0.1996, Ġuse      : 0.0101\n",
      "Layer   7:  Ġbe       : 0.7565, Ġmake     : 0.1565, Ġdo       : 0.0315\n",
      "Layer   8:  Ġbe       : 0.5986, Ġdo       : 0.2438, Ġthe      : 0.0481\n",
      "Layer   9:  Ġthe      : 0.9324, ,         : 0.0663, Ġ\"        : 0.0008\n",
      "Layer  10:  Ġthe      : 0.9999, ,         : 0.0001, Ġuse      : 0.0000\n",
      "Layer  11:  Ġthe      : 0.9956, ,         : 0.0044, Ġ\"        : 0.0000\n",
      "\n",
      "Final:      Ġplay     : 0.2032, Ġwatch    : 0.0507, Ġhave     : 0.0292\n"
     ]
    }
   ],
   "source": [
    "print(f'current token: {tokenizer.decode(inputs[\"input_ids\"][0, token_index_of_interest])}')\n",
    "print(f'target  token: {tokenizer.decode(inputs[\"input_ids\"][0, token_index_of_interest+1])}')\n",
    "\n",
    "relevant_part = inputs[\"input_ids\"][0, token_index_of_interest-1: token_index_of_interest + 2]\n",
    "print(tokenizer.decode(relevant_part.tolist()))\n",
    "print('\\n')\n",
    "\n",
    "layers_of_interest = list(range(12))\n",
    "\n",
    "# Get the layer predictions\n",
    "layer_predictions(outputs, unembed, token_index_of_interest, layers_of_interest)\n",
    "\n",
    "token_index_of_interest +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
