{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/hpc/pheno/inar/mambaforge/envs/torch39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.35s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", \n",
    "                                          torch_dtype=torch.float16,\n",
    "                                          device_map={\"\": 0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpt = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's prequel time again. In December, we'll get to 1984's \"The Terminator,\" and in January, we'll get to 1989's \"Terminator 2: Judgment Day.\"\n",
      "\n",
      "The first film, which was directed by James Cameron, is about a cyborg assassin from the future who is sent back in time to kill Sarah Connor (Linda Hamilton), the mother of the future leader of the human resistance against the machines. The second film, which was also directed by Cameron, is about the same cyborg assassin, who is sent back in time to protect Sarah's son, John Connor (\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "prompt = \"It's prequel time again. In December, we'll get to \"\n",
    "generated = pipe(prompt, streamer=TextStreamer(tokenizer), max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's prequel time again. In December, we'll get to  get more info on the upcoming sequel, and with the new trailer set to be revealed soon, we'll know about the upcoming trailer for the upcoming game. Meanwhile, let me know if you like some of the interesting characters and plot moments from the game. \n",
      "A new trailer from the game\n",
      "I've had a chance to talk to a few of the characters through the game, and they all tell me they really thought that they would have an opportunity to fight back against the Dark Lords of Draxheim. That is why this news was sent from a different perspective to them from the Dark Lord of Draxheim, to be\n"
     ]
    }
   ],
   "source": [
    "pipe_gpt = pipeline(\"text-generation\", model=model_gpt, tokenizer=tokenizer_gpt)\n",
    "prompt_gpt = \"It's prequel time again. In December, we'll get to \"\n",
    "generated_gpt = pipe_gpt(prompt_gpt, streamer=TextStreamer(tokenizer_gpt), max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mistral = generated[0]['generated_text']\n",
    "text_gpt = generated_gpt[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('It\\'s prequel time again. In December, we\\'ll get to 1984\\'s \"The Terminator,\" and in January, we\\'ll get to 1989\\'s \"Terminator 2: Judgment Day.\"\\n\\nThe first film, which was directed by James Cameron, is about a cyborg assassin from the future who is sent back in time to kill Sarah Connor (Linda Hamilton), the mother of the future leader of the human resistance against the machines. The second film, which was also directed by Cameron, is about the same cyborg assassin, who is sent back in time to protect Sarah\\'s son, John Connor (',\n",
       " 'It\\'s prequel time again. In December, we\\'ll get to \\xa0see one of the other games that\\'s not in the \"prequel\" canon. In this one,\\xa0 HEX \\xa0is now a side quest that adds a new story line to the series. So what about Episodes 9-11? Well, if you\\'ve been following the franchise for any length of time, you\\'ve probably already begun following them. I think the series should take you through some of those quests if you want to go deeper into the \"prequel\"-as in, an alternate timeline where you aren\\'t sure if you\\'re at or outside of this world and this game never really comes out. So that')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_mistral, text_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/lustre/hpc/pheno/inar/mambaforge/envs/torch39/lib/python3.9/site-packages/transformers/generation/utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt\n",
    "prompt = \"Your text prompt here.\"\n",
    "\n",
    "# Encode the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt') # Send to GPU (device 0)\n",
    "\n",
    "# Generate text with both models\n",
    "output_ids_mistral = model.generate(input_ids, max_length=50)  # Adjust max_length as needed\n",
    "output_ids_gpt = model_gpt.generate(input_ids, max_length=50)  # Adjust max_length as needed\n",
    "\n",
    "# Decode the generated ids to text\n",
    "generated_text_mistral = tokenizer.decode(output_ids_mistral[0], skip_special_tokens=True)\n",
    "generated_text_gpt = tokenizer_gpt.decode(output_ids_gpt[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def compute_loss(model, tokenizer, text):\n",
    "    # Tokenize and prepare input and labels\n",
    "    device = model.device\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "    labels = inputs.clone()\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Shift logits and labels to align for loss calculation\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Loss for Mistral generated text: 2.1475560665130615\n",
      "GPT Loss for GPT generated text: 2.790302276611328\n",
      "Mistral Loss for Mistral generated text: 0.901909351348877\n"
     ]
    }
   ],
   "source": [
    "# Compute loss for each generated text\n",
    "loss_mistral_text = compute_loss(model_gpt, tokenizer_gpt, text_mistral)  \n",
    "loss_gpt_text = compute_loss(model_gpt, tokenizer_gpt, text_gpt)  \n",
    "\n",
    "print(f\"GPT Loss for Mistral generated text: {loss_mistral_text}\")\n",
    "print(f\"GPT Loss for GPT generated text: {loss_gpt_text}\")\n",
    "\n",
    "\n",
    "loss_mistral_mistral = compute_loss(model, tokenizer, text_mistral)  \n",
    "loss_gpt_mistral = compute_loss(model, tokenizer, text_gpt)  \n",
    "\n",
    "print(f\"Mistral Loss for Mistral generated text: {loss_mistral_mistral}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for Mistral generated text: 2.46722149848938\n",
      "Loss for GPT generated text: 3.3510661125183105\n",
      "\n",
      "Mistral Loss for Mistral generated text: 1.3705629110336304\n",
      "Mistral Loss for GPT generated text: 4.150167942047119\n"
     ]
    }
   ],
   "source": [
    "# Compute loss for each generated text\n",
    "loss_mistral_text = compute_loss(model_gpt, tokenizer_gpt, generated_text_mistral)  \n",
    "loss_gpt_text = compute_loss(model_gpt, tokenizer_gpt, generated_text_gpt)  \n",
    "\n",
    "print(f\"Loss for Mistral generated text: {loss_mistral_text}\")\n",
    "print(f\"Loss for GPT generated text: {loss_gpt_text}\\n\")\n",
    "\n",
    "loss_mistral_mistral = compute_loss(model, tokenizer, generated_text_mistral)\n",
    "loss_gpt_mistral = compute_loss(model, tokenizer, generated_text_gpt)\n",
    "\n",
    "print(f\"Mistral Loss for Mistral generated text: {loss_mistral_mistral}\")\n",
    "print(f\"Mistral Loss for GPT generated text: {loss_gpt_mistral}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
